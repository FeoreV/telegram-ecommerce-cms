# Logstash Configuration for Telegram E-commerce Bot Platform
# Centralized log processing pipeline

input {
  # Backend application logs
  file {
    path => "/var/log/backend/*.log"
    start_position => "beginning"
    sincedb_path => "/usr/share/logstash/sincedb/backend"
    codec => "json"
    type => "backend"
    tags => ["backend", "application"]
  }

  # Bot service logs
  file {
    path => "/var/log/bot/*.log"
    start_position => "beginning"
    sincedb_path => "/usr/share/logstash/sincedb/bot"
    codec => "json"
    type => "bot"
    tags => ["bot", "telegram"]
  }

  # NGINX access logs
  file {
    path => "/var/log/nginx/access.log"
    start_position => "beginning"
    sincedb_path => "/usr/share/logstash/sincedb/nginx_access"
    type => "nginx_access"
    tags => ["nginx", "access"]
  }

  # NGINX error logs
  file {
    path => "/var/log/nginx/error.log"
    start_position => "beginning"
    sincedb_path => "/usr/share/logstash/sincedb/nginx_error"
    type => "nginx_error"
    tags => ["nginx", "error"]
  }

  # Docker container logs (if using Docker logging driver)
  beats {
    port => 5044
    type => "docker"
    tags => ["docker", "containers"]
  }

  # Syslog input for system logs
  syslog {
    port => 5514
    type => "syslog"
    tags => ["system", "syslog"]
  }
}

filter {
  # Add timestamp processing
  if [timestamp] {
    date {
      match => [ "timestamp", "ISO8601" ]
    }
  } else if [@timestamp] {
    # Use existing timestamp
  } else {
    # Add current timestamp if none exists
    mutate {
      add_field => { "timestamp" => "%{+YYYY-MM-dd'T'HH:mm:ss.SSSZ}" }
    }
  }

  # Process backend application logs
  if "backend" in [tags] {
    # Parse structured JSON logs
    if [message] =~ /^{.*}$/ {
      json {
        source => "message"
      }
    }

    # Extract request ID for tracing
    if [requestId] {
      mutate {
        add_field => { "trace_id" => "%{requestId}" }
      }
    }

    # Categorize log levels
    if [level] {
      mutate {
        add_field => { "log_severity" => "%{level}" }
      }
    }

    # Extract bot-specific information
    if [storeId] {
      mutate {
        add_field => { "bot_store_id" => "%{storeId}" }
      }
    }

    # Extract user information
    if [userId] {
      mutate {
        add_field => { "user_id" => "%{userId}" }
      }
    }

    # Parse API endpoint information
    if [endpoint] {
      grok {
        match => { "endpoint" => "(?<http_method>GET|POST|PUT|DELETE|PATCH) (?<api_path>/api/[^\\s]*)" }
      }
    }
  }

  # Process bot service logs
  if "bot" in [tags] {
    # Parse bot-specific fields
    if [botToken] {
      # Hash sensitive bot tokens
      fingerprint {
        source => ["botToken"]
        target => "bot_token_hash"
        method => "SHA256"
        key => "logstash"
      }
      mutate {
        remove_field => ["botToken"]
      }
    }

    # Extract Telegram user information
    if [telegramUser] {
      json {
        source => "telegramUser"
        target => "telegram_user"
      }
    }

    # Extract message statistics
    if [messageCount] {
      mutate {
        convert => { "messageCount" => "integer" }
      }
    }
  }

  # Process NGINX access logs
  if "nginx" in [tags] and "access" in [tags] {
    grok {
      match => { "message" => "%{COMBINEDAPACHELOG}" }
    }

    # Parse response time if available
    grok {
      match => { "message" => "request_time=(?<request_time>[0-9.]+)" }
    }

    # Convert fields to appropriate types
    mutate {
      convert => { 
        "response" => "integer"
        "bytes" => "integer" 
        "request_time" => "float"
      }
    }

    # Add response categorization
    if [response] {
      if [response] >= 200 and [response] < 300 {
        mutate { add_field => { "response_category" => "success" } }
      } else if [response] >= 300 and [response] < 400 {
        mutate { add_field => { "response_category" => "redirect" } }
      } else if [response] >= 400 and [response] < 500 {
        mutate { add_field => { "response_category" => "client_error" } }
      } else if [response] >= 500 {
        mutate { add_field => { "response_category" => "server_error" } }
      }
    }
  }

  # Process NGINX error logs
  if "nginx" in [tags] and "error" in [tags] {
    grok {
      match => { "message" => "(?<timestamp>\\d{4}/\\d{2}/\\d{2} \\d{2}:\\d{2}:\\d{2}) \\[(?<log_level>\\w+)\\] (?<pid>\\d+)#(?<tid>\\d+): (?<error_message>.*)" }
    }
  }

  # General enrichment
  mutate {
    add_field => { 
      "environment" => "${ENVIRONMENT:production}"
      "service_version" => "${SERVICE_VERSION:unknown}"
      "hostname" => "%{host}"
    }
  }

  # GeoIP enrichment for client IPs
  if [clientip] {
    geoip {
      source => "clientip"
      target => "geoip"
    }
  }

  # Remove empty fields
  if [message] == "" or [message] == "-" {
    mutate {
      remove_field => ["message"]
    }
  }

  # Add unique document ID
  fingerprint {
    source => ["@timestamp", "host", "message", "type"]
    target => "[@metadata][document_id]"
    method => "SHA1"
  }
}

output {
  # Output to Elasticsearch
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "telegram-ecommerce-logs-%{+YYYY.MM.dd}"
    document_id => "%{[@metadata][document_id]}"
    template_name => "telegram-ecommerce"
    template_pattern => "telegram-ecommerce-logs-*"
    template => {
      "settings" => {
        "index" => {
          "number_of_shards" => 2,
          "number_of_replicas" => 1,
          "refresh_interval" => "10s"
        }
      },
      "mappings" => {
        "properties" => {
          "@timestamp" => { "type" => "date" },
          "level" => { "type" => "keyword" },
          "message" => { "type" => "text", "analyzer" => "standard" },
          "service" => { "type" => "keyword" },
          "environment" => { "type" => "keyword" },
          "hostname" => { "type" => "keyword" },
          "trace_id" => { "type" => "keyword" },
          "user_id" => { "type" => "keyword" },
          "bot_store_id" => { "type" => "keyword" },
          "response" => { "type" => "integer" },
          "request_time" => { "type" => "float" },
          "response_category" => { "type" => "keyword" },
          "geoip" => {
            "properties" => {
              "location" => { "type" => "geo_point" },
              "country_name" => { "type" => "keyword" },
              "city_name" => { "type" => "keyword" }
            }
          }
        }
      }
    }
  }

  # Output critical errors to separate index
  if [level] == "error" or [log_level] == "error" or [response] >= 500 {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      index => "telegram-ecommerce-errors-%{+YYYY.MM.dd}"
      document_id => "%{[@metadata][document_id]}-error"
    }
  }

  # Debug output to stdout in development
  if "${ENVIRONMENT}" != "production" {
    stdout {
      codec => rubydebug
    }
  }

  # Send alerts for critical errors
  if [level] == "fatal" or ([response] and [response] >= 500 and [response_category] == "server_error") {
    http {
      url => "${ALERT_WEBHOOK_URL}"
      http_method => "post"
      format => "json"
      mapping => {
        "title" => "Critical Error in Telegram E-commerce Platform"
        "message" => "%{message}"
        "service" => "%{type}"
        "timestamp" => "%{@timestamp}"
        "hostname" => "%{hostname}"
        "level" => "%{level}"
      }
    }
  }
}
